# -*- coding: utf-8 -*-
"""assignment_diffusion_a.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BA6kL9_6JI3yYe9q1CrHvJdMPRAD1yk5

# DDPM (Unconditional) on MNIST - Part A
This notebook implements a compact Denoising Diffusion Probabilistic Model (DDPM) Trained on MNIST.
GOAL:
- Implement forward noising and the reverse denoising model
- Train a lighweight UNET-Style denoiser that predicts the added gaussian noise.
- save training loss and example generrations.

structure:
1. Setup (Install, Imports)
2. Utilities (save/display  helpers)
3. Model( small Unet with timesteps embeddings)
4. Diffusion math and sampling
5. Training Loop _ periodic sampling
6. visualize results and suggestions for experiments
"""

! pip install -q torch torchvision matplotlib tqdm

import os
RESULTS_DIR = "/content/results/uncond"
os.makedirs(RESULTS_DIR, exist_ok=True)
print("Ready - Results will be saved to", RESULTS_DIR)

# This cell imports libraries and selects GPU if available. The rest of notebook uses `device`.

import math, random, time
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch import optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms, utils
from tqdm import tqdm

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

## Helpers:
### - save_grid(tensor, path): write image grid to disk (assumes values in [-1,1])
### - show_grid(tensor): display inline
# --These are small and commonly reused.

def save_grid(x: torch.Tensor, path: str, nrow: int = 8):
  # X: (B,C,H,W) with values roughly in [-1,1]
  x = (x.clamp(-1, 1) + 1) / 2 #map to [0,1]
  utils.save_image(x, path, nrow=nrow)

def show_grid(x: torch.Tensor, nrow: int = 8, figsize=(5,5)):
  x = (x.clamp(-1,1)+1)/2
  grid = utils.make_grid(x, nrow=nrow)
  npimg = grid.cpu().numpy()
  plt.figure(figsize=figsize)
  plt.imshow(np.transpose(npimg, (1,2,0)), interpolation="nearest")
  plt.axis("off")
  plt.show()



""" Model choices (compact and fast for MNIST):
 - A small encoder-decoder (UNet-ish) with two downsampling steps and two upsampling steps.
 - Sinusoidal timestep embedding (like positional encoding).
 - The network predicts noise ε; so final activation is linear.
 This design balances clarity and trainability on limited hardware.

"""

# Compact UNet-like denoiser
class SmallConv(nn.Module):
    def __init__(self, in_ch, out_ch):
        super().__init__()
        self.conv = nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1)
        self.bn = nn.BatchNorm2d(out_ch)
        self.act = nn.SiLU()

    def forward(self, x):
        return self.act(self.bn(self.conv(x)))

def sin_pos_emb(t, dim):
    # t: (B,) long tensor; dim: output dim
    half = dim // 2
    freqs = torch.exp(-math.log(10000) * torch.arange(0, half, device=t.device).float() / half)
    args = t.float().unsqueeze(1) * freqs.unsqueeze(0)
    emb = torch.cat([torch.sin(args), torch.cos(args)], dim=-1)
    if dim % 2 == 1:
        emb = F.pad(emb, (0,1), value=0.0)
    return emb  # (B, dim)

class BaseUNet(nn.Module):
    def __init__(self, in_ch=1, base_channels=48, t_emb_dim=128):
        super().__init__()
        self.inc = SmallConv(in_ch, base_channels)                # same res
        self.down1 = SmallConv(base_channels, base_channels*2)    # /2
        self.down2 = SmallConv(base_channels*2, base_channels*4)  # /4

        self.mid = SmallConv(base_channels*4, base_channels*4)

        self.up2 = nn.ConvTranspose2d(base_channels*4, base_channels*2, kernel_size=2, stride=2)
        self.up_conv2 = SmallConv(base_channels*4, base_channels*2)
        self.up1 = nn.ConvTranspose2d(base_channels*2, base_channels, kernel_size=2, stride=2)
        self.up_conv1 = SmallConv(base_channels*2, base_channels)

        self.out = nn.Conv2d(base_channels, in_ch, kernel_size=1)

        # timestep MLP: project positional embedding into channel space
        self.t_proj = nn.Linear(t_emb_dim, t_emb_dim)
        self.t_mlp = nn.Sequential(
            nn.Linear(t_emb_dim, t_emb_dim),
            nn.SiLU(),
            nn.Linear(t_emb_dim, base_channels*4)
        )

    def forward(self, x, t):
        # x: (B,C,H,W), t: (B,) long tensor
        temb = sin_pos_emb(t, self.t_proj.in_features)
        temb = self.t_proj(temb)
        temb_chan = self.t_mlp(temb)  # (B, base_ch*4)

        x1 = self.inc(x)                     # B,base,H,W
        x2 = F.avg_pool2d(x1, kernel_size=2) # down (H/2, W/2)
        x2 = self.down1(x2)
        x3 = F.avg_pool2d(x2, kernel_size=2) # down (H/4, W/4)
        x3 = self.down2(x3)

        # add temb (broadcast spatially)
        b,c,h,w = x3.shape
        x3 = x3 + temb_chan.view(b, c, 1, 1)
        m = self.mid(x3)

        u2 = self.up2(m)
        u2 = torch.cat([u2, x2], dim=1)
        u2 = self.up_conv2(u2)
        u1 = self.up1(u2)
        u1 = torch.cat([u1, x1], dim=1)
        u1 = self.up_conv1(u1)

        return self.out(u1)  # predict noise (epsilon), same shape as x



"""now let's implement:
- linear beta schedule β_t (linearly increasing)
- precomputed alpha and alpha_cumprod arrays for fast sampling
- q_sample: generate x_t from x_0 and injected noise
- p_sample_loop: iterative reverse sampling using the trained model that predicts ε
Equations follow Ho et al. (DDPM) simplified objective (predict ε).

"""

class SimpleSchedule:
    def __init__(self, timesteps=200, beta_start=1e-4, beta_end=2e-2, device='cpu'):
        self.T = timesteps
        self.device = device
        self.betas = torch.linspace(beta_start, beta_end, timesteps, device=device)
        self.alphas = 1.0 - self.betas
        self.alpha_cum = torch.cumprod(self.alphas, dim=0)
        self.sqrt_alpha_cum = torch.sqrt(self.alpha_cum)
        self.sqrt_one_minus_alpha_cum = torch.sqrt(1 - self.alpha_cum)

    def q_sample(self, x0, t, noise=None):
        # x0: (B,C,H,W), t: (B,) long tensor with values [0..T-1]
        if noise is None:
            noise = torch.randn_like(x0)
        a_t = self.sqrt_alpha_cum[t].view(-1,1,1,1)
        b_t = self.sqrt_one_minus_alpha_cum[t].view(-1,1,1,1)
        return a_t * x0 + b_t * noise

    @torch.no_grad()
    def p_sample_loop(self, model, shape, device, y=None):
        x = torch.randn(shape).to(device)
        for i in reversed(range(self.T)):
            t_scalar = i
            b = x.shape[0]
            t_batch = torch.tensor([t_scalar], device=device).repeat(b)
            # model predicts noise
            eps_pred = model(x, t_batch)
            beta_t = self.betas[t_scalar]
            alpha_t = self.alphas[t_scalar]
            alpha_cum_t = self.alpha_cum[t_scalar]

            coef1 = 1.0 / torch.sqrt(alpha_t)
            coef2 = beta_t / torch.sqrt(1.0 - alpha_cum_t)
            mean = coef1 * (x - coef2 * eps_pred)

            if i == 0:
                x = mean
            else:
                noise = torch.randn_like(x)
                sigma = torch.sqrt(beta_t)
                x = mean + sigma * noise
        return x



"""Training procedure (simple and clear):
- Data: MNIST normalized to [-1,1]
- In each step:
  - sample random timesteps t ~ Uniform(0, T-1)
  - sample noise ε ~ N(0,I)
  - form x_t = sqrt_alpha_cum[t] * x0 + sqrt(1 - alpha_cum[t]) * ε
  - predict ε̂ = model(x_t, t)
  - loss = MSE(ε̂, ε)
- Periodically save sampled grids and checkpoints
- Keep the network small so it finishes in reasonable time on Colab GPU

"""

def train_ddpm(
    epochs=6,
    batch_size=128,
    lr=2e-4,
    timesteps=200,
    base_ch=48,
    results_dir=RESULTS_DIR
):
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Lambda(lambda t: t * 2.0 - 1.0)  # [0,1] -> [-1,1]
    ])
    ds = datasets.MNIST(root="/content/data", train=True, transform=transform, download=True)
    dl = DataLoader(ds, batch_size=batch_size, shuffle=True, num_workers=2)

    model = BaseUNet(in_ch=1, base_channels=base_ch, t_emb_dim=128).to(device)
    schedule = SimpleSchedule(timesteps=timesteps, device=device)
    opt = optim.Adam(model.parameters(), lr=lr)
    mse = nn.MSELoss()

    losses = []
    step = 0
    for ep in range(epochs):
        pbar = tqdm(dl, desc=f"Epoch {ep+1}/{epochs}")
        for x, _ in pbar:
            x = x.to(device)
            b = x.shape[0]
            t = torch.randint(0, timesteps, (b,), device=device).long()
            noise = torch.randn_like(x)
            x_t = schedule.q_sample(x, t, noise=noise)

            pred = model(x_t, t)
            loss = mse(pred, noise)

            opt.zero_grad()
            loss.backward()
            opt.step()

            losses.append(loss.item())
            pbar.set_postfix({'loss': loss.item()})

            # quick sample snapshots
            if step % 500 == 0:
                model.eval()
                with torch.no_grad():
                    samples = schedule.p_sample_loop(model, (64,1,28,28), device)
                    save_grid(samples, os.path.join(results_dir, f"sample_{step}.png"))
                model.train()
            step += 1

        # save checkpoint each epoch
        ckpt = {'model': model.state_dict(), 'opt': opt.state_dict(), 'ep': ep}
        torch.save(ckpt, os.path.join(results_dir, f"ckpt_ep{ep}.pth"))

    # final samples and loss curve
    model.eval()
    with torch.no_grad():
        final = schedule.p_sample_loop(model, (64,1,28,28), device)
        save_grid(final, os.path.join(results_dir, "final_samples.png"))

    # smooth and save loss
    if len(losses) > 50:
        smooth = np.convolve(losses, np.ones(50)/50, mode='valid')
    else:
        smooth = losses
    plt.figure(figsize=(6,3))
    plt.plot(smooth)
    plt.title("Training loss (smoothed)")
    plt.xlabel("steps")
    plt.ylabel("MSE")
    plt.tight_layout()
    plt.savefig(os.path.join(results_dir, "loss_curve.png"))
    plt.close()

    print("Done. Results in:", results_dir)
    return model, schedule, losses

# Run training (recommended defaults for Colab GPU)
# If you are on CPU, consider epochs=3, batch_size=64, timesteps=100 for speed.
model, schedule, losses = train_ddpm(
    epochs=6,
    batch_size=128,
    lr=2e-4,
    timesteps=200,
    base_ch=48,
    results_dir=RESULTS_DIR
)

# Display the final sample grid and the training loss curve saved in results folder.
from IPython.display import display
final_path = os.path.join(RESULTS_DIR, "final_samples.png")
loss_path = os.path.join(RESULTS_DIR, "loss_curve.png")

if os.path.exists(final_path):
    display(Image.open(final_path))
else:
    print("No final samples saved yet.")

if os.path.exists(loss_path):
    display(Image.open(loss_path))
else:
    print("No loss plot saved yet.")

# === Experiment
import shutil, time

# Make sure RESULTS_DIR root exists
ROOT = "/content/results/uncond"
os.makedirs(ROOT, exist_ok=True)

def run_experiments_timesteps(timesteps_list=[50,200,1000],
                              epochs=4,
                              batch_size=128,
                              lr=2e-4,
                              base_ch=48,
                              short_run=True):
    """
    Runs train_ddpm for each timesteps value and saves outputs under:
      /content/results/uncond/exp_timesteps_<T>/
    short_run: if True uses fewer epochs/steps for fast comparison (useful on CPU)
    """
    experiments = []
    for T in timesteps_list:
        exp_name = f"exp_timesteps_{T}"
        exp_dir = os.path.join(ROOT, exp_name)
        # ensure clean folder for each experiment
        if os.path.exists(exp_dir):
            print("Overwriting existing", exp_dir)
            shutil.rmtree(exp_dir)
        os.makedirs(exp_dir, exist_ok=True)

        print("\n=== Running experiment:", exp_name, "===")
        # call the same train function but pass results_dir=exp_dir
        # Using shorter epochs if short_run True to save time
        e = epochs if not short_run else max(1, epochs//2)
        start = time.time()
        model, schedule, losses = train_ddpm(
            epochs=e,
            batch_size=batch_size,
            lr=lr,
            timesteps=T,
            base_ch=base_ch,
            results_dir=exp_dir
        )
        elapsed = time.time() - start
        print(f"Finished {exp_name} in {elapsed/60:.2f} minutes; saved to {exp_dir}")
        experiments.append((exp_name, exp_dir, losses))
    return experiments

# Example: run timesteps experiments
experiments = run_experiments_timesteps(timesteps_list=[50,200], epochs=4, batch_size=128, lr=2e-4, base_ch=48, short_run=True)

# experiments += run_experiments_timesteps(timesteps_list=[1000], epochs=4, batch_size=128, lr=2e-4, base_ch=48, short_run=False)

"""***short_run=True*** halves epochs so you get indicative results faster. For final comparison, run with ***short_run=False*** (full epochs).

Each experiment will save ***final_samples.png***, intermediate ***sample_.png***, ***loss_curve.png***, and checkpoints in its experiment folder.
"""

# === Visualization: load experiments list and plot side-by-side ===
from PIL import Image
import matplotlib.pyplot as plt

def compare_experiments(experiments):
    """
    experiments: list of tuples (exp_name, exp_dir, losses)
    Displays side-by-side sample grids and overlays loss curves.
    """
    # 1) Display sample images side-by-side
    fig_samples = plt.figure(figsize=(4*len(experiments), 4))
    for i, (name, d, losses) in enumerate(experiments, 1):
        img_path = os.path.join(d, "final_samples.png")
        if os.path.exists(img_path):
            ax = fig_samples.add_subplot(1, len(experiments), i)
            img = Image.open(img_path)
            ax.imshow(img)
            ax.set_title(name)
            ax.axis('off')
        else:
            print("Missing final_samples.png for", name)
    plt.tight_layout()
    plt.show()

    # 2) Plot loss curves (smoothed) on single plot
    plt.figure(figsize=(8,4))
    for name, d, losses in experiments:
        if losses is None or len(losses) == 0:
            # try to load loss_curve.png presence, otherwise skip
            lc_path = os.path.join(d, "loss_curve.png")
            if os.path.exists(lc_path):
                print("loss_curve image exists for", name)
            else:
                print("No loss array or loss_curve.png for", name)
            continue
        if len(losses) > 50:
            smooth = np.convolve(losses, np.ones(50)/50, mode='valid')
        else:
            smooth = losses
        plt.plot(smooth, label=name)
    plt.legend()
    plt.xlabel("steps (smoothed)")
    plt.ylabel("MSE loss")
    plt.title("Training loss comparison")
    plt.tight_layout()
    plt.show()

# Call visualization (use the 'experiments' returned by the runner)
compare_experiments(experiments)

# Optional: create zip of results to download
import shutil
zip_dest = "/content/ddpm_partA_results.zip"
shutil.make_archive(zip_dest.replace('.zip',''), 'zip', "/content/results/uncond")
print("Zipped results to", zip_dest)


# -*- coding: utf-8 -*-
"""assignment_diffusion_b.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1siTUoxS1vPfWr9xbDuh9vjN7CJwUK6zh

# Part B — Conditional DDPM on MNIST

Goal:
- Extend the DDPM to generate MNIST digits conditioned on class labels (0–9).
- Use a label embedding that is added to the timestep embedding, so the denoiser receives class information.
- Train and save conditional sample grids for multiple classes (e.g., 0, 3, 7).
- Keep the model compact so training is reasonable on Colab.

Notebook structure:
1. Setup & imports
2. Utilities (save/display)
3. Conditional UNet (label embedding + timestep embedding)
4. Diffusion schedule with conditional sampling
5. Training loop using labels
6. Save & display conditional samples
7. Experiment suggestions & report notes
"""

!pip install -q torch torchvision matplotlib tqdm

import os
RESULTS_DIR = "/content/results/cond"
os.makedirs(RESULTS_DIR, exist_ok=True)
print("Results will be saved to:", RESULTS_DIR)

"""Imports, reproducibility note, and device selection.

"""

import math, random, time
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch import optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms, utils
from tqdm import tqdm

# reproducible-ish
torch.manual_seed(42)
np.random.seed(42)
random.seed(42)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)

## Small helpers reused from Part A:
#- save_grid: save tensors ([-1,1]) to PNG
#- show_grid: display inline

def save_grid(x: torch.Tensor, path: str, nrow: int = 8):
    x = (x.clamp(-1, 1) + 1) / 2
    utils.save_image(x, path, nrow=nrow)

def show_grid(x: torch.Tensor, nrow: int = 8, figsize=(5,5)):
    x = (x.clamp(-1, 1) + 1) / 2
    grid = utils.make_grid(x, nrow=nrow)
    npimg = grid.cpu().numpy()
    plt.figure(figsize=figsize)
    plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')
    plt.axis('off')
    plt.show()

"""Conditioning approach (simple and effective for MNIST):
- Learn an embedding for each class label (Embedding(num_classes, emb_dim)).
- Add the label embedding to the sinusoidal timestep embedding before projecting into channel space.
- The network therefore receives a combined embedding that carries both time and class information.
This is easy to explain and implement and works well on MNIST.

"""

# Conditional UNet-like denoiser: accepts labels y
class SmallBlock(nn.Module):
    def __init__(self, in_ch, out_ch):
        super().__init__()
        self.conv = nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1)
        self.bn = nn.BatchNorm2d(out_ch)
        self.act = nn.SiLU()
    def forward(self, x):
        return self.act(self.bn(self.conv(x)))

def sin_embed(t, dim):
    half = dim // 2
    freqs = torch.exp(-math.log(10000) * torch.arange(0, half, device=t.device).float() / half)
    args = t.float().unsqueeze(1) * freqs.unsqueeze(0)
    emb = torch.cat([torch.sin(args), torch.cos(args)], dim=-1)
    if dim % 2 == 1:
        emb = F.pad(emb, (0,1), value=0.0)
    return emb

class ConditionalUNet(nn.Module):
    def __init__(self, in_ch=1, base_ch=48, t_emb_dim=128, num_classes=10):
        super().__init__()
        # encoder
        self.inc = SmallBlock(in_ch, base_ch)
        self.down1 = SmallBlock(base_ch, base_ch*2)
        self.down2 = SmallBlock(base_ch*2, base_ch*4)
        # mid
        self.mid = SmallBlock(base_ch*4, base_ch*4)
        # decoder
        self.up2 = nn.ConvTranspose2d(base_ch*4, base_ch*2, kernel_size=2, stride=2)
        self.up_conv2 = SmallBlock(base_ch*4, base_ch*2)
        self.up1 = nn.ConvTranspose2d(base_ch*2, base_ch, kernel_size=2, stride=2)
        self.up_conv1 = SmallBlock(base_ch*2, base_ch)
        self.out = nn.Conv2d(base_ch, in_ch, kernel_size=1)

        # embeddings
        self.t_proj = nn.Linear(t_emb_dim, t_emb_dim)
        self.t_mlp = nn.Sequential(
            nn.Linear(t_emb_dim, t_emb_dim),
            nn.SiLU(),
            nn.Linear(t_emb_dim, base_ch*4)
        )
        self.label_emb = nn.Embedding(num_classes, t_emb_dim)  # label conditioning

    def forward(self, x, t, y):
        """
        x: (B,C,H,W), t: (B,) long tensor, y: (B,) long tensor (labels)
        """
        temb = sin_embed(t, self.t_proj.in_features)
        temb = self.t_proj(temb)
        yemb = self.label_emb(y)
        temb = temb + yemb                # combine time + label
        temb_chan = self.t_mlp(temb)      # to channel space

        x1 = self.inc(x)
        x2 = F.avg_pool2d(x1, 2)
        x2 = self.down1(x2)
        x3 = F.avg_pool2d(x2, 2)
        x3 = self.down2(x3)

        # add embedding to deepest feature map
        b,c,h,w = x3.shape
        x3 = x3 + temb_chan.view(b, c, 1, 1)
        m = self.mid(x3)

        u2 = self.up2(m)
        u2 = torch.cat([u2, x2], dim=1)
        u2 = self.up_conv2(u2)
        u1 = self.up1(u2)
        u1 = torch.cat([u1, x1], dim=1)
        u1 = self.up_conv1(u1)
        return self.out(u1)  # predicts noise epsilon



"""Diffusion schedule utilities:
- Same linear beta schedule as Part A.
- The sampling loop accepts labels `y` so it can produce class-conditional images: each sampling step passes the same label tensor to the model.

"""

class CondSchedule:
    def __init__(self, T=200, beta_start=1e-4, beta_end=2e-2, device='cpu'):
        self.T = T
        self.device = device
        self.betas = torch.linspace(beta_start, beta_end, T, device=device)
        self.alphas = 1.0 - self.betas
        self.alpha_cum = torch.cumprod(self.alphas, dim=0)
        self.sqrt_alpha_cum = torch.sqrt(self.alpha_cum)
        self.sqrt_one_minus_alpha_cum = torch.sqrt(1 - self.alpha_cum)

    def q_sample(self, x0, t, noise=None):
        if noise is None:
            noise = torch.randn_like(x0)
        a = self.sqrt_alpha_cum[t].view(-1,1,1,1)
        b = self.sqrt_one_minus_alpha_cum[t].view(-1,1,1,1)
        return a * x0 + b * noise

    @torch.no_grad()
    def p_sample_loop(self, model, shape, device, y):
        """
        shape: (B,C,H,W)
        y: (B,) labels to condition on
        """
        x = torch.randn(shape).to(device)
        for i in reversed(range(self.T)):
            b = x.shape[0]
            t_batch = torch.tensor([i], device=device).repeat(b)
            eps_pred = model(x, t_batch, y)
            beta_t = self.betas[i]
            alpha_t = self.alphas[i]
            alpha_cum_t = self.alpha_cum[i]

            coef1 = 1.0 / torch.sqrt(alpha_t)
            coef2 = beta_t / torch.sqrt(1.0 - alpha_cum_t)
            mean = coef1 * (x - coef2 * eps_pred)

            if i == 0:
                x = mean
            else:
                noise = torch.randn_like(x)
                sigma = torch.sqrt(beta_t)
                x = mean + sigma * noise
        return x

"""Training loop summary:
- Load MNIST with normalization to [-1,1].
- For each batch, sample t and noise and compute x_t = q_sample(x0,t).
- Model predicts ε̂ = model(x_t, t, y) where y are the labels from dataset.
- Loss = MSE(ε̂, ε).
- Periodically sample grids conditioned on a set of labels for monitoring.
- At end, save sample grids for at least 3 different classes (e.g., 0, 3, 7).

"""

def train_conditional_ddpm(
    epochs=6,
    batch_size=128,
    lr=2e-4,
    timesteps=200,
    base_ch=48,
    num_classes=10,
    results_dir=RESULTS_DIR
):
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Lambda(lambda t: t * 2.0 - 1.0)
    ])
    ds = datasets.MNIST(root="/content/data", train=True, transform=transform, download=True)
    dl = DataLoader(ds, batch_size=batch_size, shuffle=True, num_workers=2)

    model = ConditionalUNet(in_ch=1, base_ch=base_ch, t_emb_dim=128, num_classes=num_classes).to(device)
    schedule = CondSchedule(T=timesteps, device=device)
    opt = optim.Adam(model.parameters(), lr=lr)
    mse = nn.MSELoss()

    losses = []
    step = 0
    for ep in range(epochs):
        pbar = tqdm(dl, desc=f"Epoch {ep+1}/{epochs}")
        for x, labels in pbar:
            x = x.to(device)
            labels = labels.to(device)
            b = x.shape[0]
            t = torch.randint(0, timesteps, (b,), device=device).long()
            noise = torch.randn_like(x)
            x_t = schedule.q_sample(x, t, noise=noise)

            pred = model(x_t, t, labels)
            loss = mse(pred, noise)

            opt.zero_grad()
            loss.backward()
            opt.step()

            losses.append(loss.item())
            pbar.set_postfix({'loss': loss.item()})

            if step % 500 == 0:
                model.eval()
                with torch.no_grad():
                    # sample one example per label 0..7 repeated to make 64
                    sample_labels = torch.arange(0, 8, device=device).long().repeat_interleave(8)[:64]
                    samples = schedule.p_sample_loop(model, (64,1,28,28), device, sample_labels)
                    save_grid(samples, os.path.join(results_dir, f"sample_{step}.png"))
                model.train()
            step += 1

        # save checkpoint
        ckpt = {'model': model.state_dict(), 'opt': opt.state_dict(), 'ep': ep}
        torch.save(ckpt, os.path.join(results_dir, f"ckpt_ep{ep}.pth"))

    # Final conditional samples for at least 3 classes
    model.eval()
    with torch.no_grad():
        for cls in [0, 3, 7]:
            lbls = torch.full((64,), cls, dtype=torch.long, device=device)
            samples = schedule.p_sample_loop(model, (64,1,28,28), device, lbls)
            save_grid(samples, os.path.join(results_dir, f"samples_class_{cls}.png"))
    # Save loss curve
    if len(losses) > 50:
        smooth = np.convolve(losses, np.ones(50)/50, mode='valid')
    else:
        smooth = losses
    plt.figure(figsize=(6,3))
    plt.plot(smooth)
    plt.title("Training loss (smoothed)")
    plt.xlabel("steps")
    plt.ylabel("MSE")
    plt.tight_layout()
    plt.savefig(os.path.join(results_dir, "loss_curve.png"))
    plt.close()

    print("Conditional training complete. Results in:", results_dir)
    return model, schedule, losses

# Run the conditional training
# If on CPU, reduce epochs or timesteps for speed.
cond_model, cond_schedule, cond_losses = train_conditional_ddpm(
    epochs=6,
    batch_size=128,
    lr=2e-4,
    timesteps=200,
    base_ch=48,
    num_classes=10,
    results_dir=RESULTS_DIR
)



"""Display the class-conditional grids saved after training. Look for files:
- results/cond/samples_class_0.png
- results/cond/samples_class_3.png
- results/cond/samples_class_7.png
And a general progress snapshot sample_*.png images.

"""

from IPython.display import display

for cls in [0, 3, 7]:
    p = os.path.join(RESULTS_DIR, f"samples_class_{cls}.png")
    if os.path.exists(p):
        print("Class", cls)
        display(Image.open(p))
    else:
        print("Not found:", p)

# show loss curve
loss_path = os.path.join(RESULTS_DIR, "loss_curve.png")
if os.path.exists(loss_path):
    display(Image.open(loss_path))
else:
    print("Loss curve not found yet.")

import shutil
zip_dest = "/content/ddpm_partB_results.zip"
shutil.make_archive(zip_dest.replace('.zip',''), 'zip', "/content/results/cond")
print("Zipped results to", zip_dest)


{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdbd1d91-e192-4929-a2b4-b06a9980bffe",
   "metadata": {},
   "source": [
    "## IMPLEMENTING A SIMPLIFIED ATTENTION MECHANISM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b023fc62-7222-44ea-8679-020024d1314a",
   "metadata": {},
   "source": [
    "#### Consider the following input sentence, which has already been embedded into 3- dimensional vectors.\n",
    "\n",
    "#### We choose a small embedding dimension for illustration purposes to ensure it fits on the page without line breaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "052b4d70-b0f4-4d8b-83a7-b2d186aa7ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ef84d95-c1ec-4e94-93d0-3c4eb867a1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# # Corresponding words\n",
    "# words = ['Your', 'journey', 'starts', 'with', 'one', 'step']\n",
    "\n",
    "# # Extract x, y, z coordinates\n",
    "# x_coords = inputs[:, 0].numpy()\n",
    "# y_coords = inputs[:, 1].numpy()\n",
    "# z_coords = inputs[:, 2].numpy()\n",
    "\n",
    "# # Create 3D plot\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# # Plot each point and annotate with corresponding word\n",
    "# for x, y, z, word in zip(x_coords, y_coords, z_coords, words):\n",
    "#     ax.scatter(x, y, z)\n",
    "#     ax.text(x, y, z, word, fontsize=10)\n",
    "\n",
    "# # Set labels for axes\n",
    "# ax.set_xlabel('X')\n",
    "# ax.set_ylabel('Y')\n",
    "# ax.set_zlabel('Z')\n",
    "\n",
    "# plt.title('3D Plot of Word Embeddings')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae762d66-4e98-4fbb-b739-d7fde6dd2587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create 3D plot with vectors from origin to each point, using different colors\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# # Define a list of colors for the vectors\n",
    "# colors = ['r', 'g', 'b', 'c', 'm', 'y']\n",
    "\n",
    "# # Plot each vector with a different color and annotate with the corresponding word\n",
    "# for (x, y, z, word, color) in zip(x_coords, y_coords, z_coords, words, colors):\n",
    "#     # Draw vector from origin to the point (x, y, z) with specified color and smaller arrow length ratio\n",
    "#     ax.quiver(0, 0, 0, x, y, z, color=color, arrow_length_ratio=0.05)\n",
    "#     ax.text(x, y, z, word, fontsize=10, color=color)\n",
    "\n",
    "# # Set labels for axes\n",
    "# ax.set_xlabel('X')\n",
    "# ax.set_ylabel('Y')\n",
    "# ax.set_zlabel('Z')\n",
    "\n",
    "# # Set plot limits to keep arrows within the plot boundaries\n",
    "# ax.set_xlim([0, 1])\n",
    "# ax.set_ylim([0, 1])\n",
    "# ax.set_zlim([0, 1])\n",
    "\n",
    "# plt.title('3D Plot of Word Embeddings with Colored Vectors')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "286e766e-c868-49ad-a35c-f7a63b3623d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]  # 2nd input token is the query\n",
    "\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, query) # Dot product(transpose not necessary)\n",
    "\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3becb6ce-f5c4-4799-bbca-020c0e5858e9",
   "metadata": {},
   "source": [
    "#### the next step, we normalize each of attention scores that we computed previously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dff59bf-16c5-4a0c-8ddb-44a0ba892f57",
   "metadata": {},
   "source": [
    "The main goal behind the normalization is to obtain attention weights that sum up to 1.\n",
    "\n",
    "This normalization is a convention that is useful for interpretation and for maintaining training stability in an LLM.\n",
    "\n",
    "Here's a straightforward method for achieving this normalization step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2cd1ee49-3e6a-4dde-a19b-c399ed4b6b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum: tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "\n",
    "print(\"Attention weights:\", attn_weights_2_tmp)\n",
    "print(\"Sum:\", attn_weights_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dc411b-5cfb-4bc0-befe-419797c84683",
   "metadata": {},
   "source": [
    "#### In practice, it's more common and advisable to use the softmax function for normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7600d0d4-f431-4fd7-9f50-8e0572b0c8ec",
   "metadata": {},
   "source": [
    "This approach is better at managing extreme values and offers more favorable gradient properties during training.\n",
    "\n",
    "Below is a basic implementation of the softmax function for normalizing the attention scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d12909dc-5470-4c89-af53-7a9925697993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "\n",
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "\n",
    "print(\"Attention weights:\", attn_weights_2_naive)\n",
    "print(\"Sum:\", attn_weights_2_naive.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39528951-8c17-460b-a79c-a1b6a497d43e",
   "metadata": {},
   "source": [
    "As the output shows, the softmax function also meets the objective and normalizes the attention weights such that they sum to 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b37e5de-c1e2-49ca-931a-afa12f12c094",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "In addition, the softmax function ensures that the attention weights are always positive.\n",
    "This makes the output interpretable as probabilities or relative importance, where higher\n",
    "weights indicate greater importance.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b99656-3c56-47d3-b6f5-49623d75f8dd",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Note that this naive softmax implementation (softmax_naive) may encounter numerical\n",
    "instability problems, such as overflow and underflow, when dealing with large or small input\n",
    "values. \n",
    "\n",
    "Therefore, in practice, it's advisable to use the PyTorch implementation of softmax,\n",
    "which has been extensively optimized for performance:\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7051275-e408-44a4-9338-11339a16851e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "print(\"Attention weights:\", attn_weights_2)\n",
    "print(\"Sum:\", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58557448-c7c3-4b16-9557-158a080f8291",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "In this case, we can see that it yields the same results as our previous softmax_naive\n",
    "function:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f2729a-3680-44cd-af20-acb5d887d802",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "The context vector z(2)is calculated as a weighted sum of all input\n",
    "vectors. \n",
    "\n",
    "This involves multiplying each input vector by its corresponding attention weight:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5017d20-d3c1-4108-9dc9-104ca2fdf71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1] # 2nd input token is the query\n",
    "\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i,x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i]*x_i\n",
    "\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "33ef5754-25ce-407e-bfe8-92a2be629845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# inputs = torch.tensor(\n",
    "#   [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "#    [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "#    [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "#    [0.22, 0.58, 0.33], # with     (x^4)\n",
    "#    [0.77, 0.25, 0.10], # one      (x^5)\n",
    "#    [0.05, 0.80, 0.55], # step     (x^6)\n",
    "#    [0.4419, 0.6515, 0.5683]]\n",
    "# )\n",
    "\n",
    "# # Corresponding words\n",
    "# words = ['Your', 'journey', 'starts', 'with', 'one', 'step', 'journey-context']\n",
    "\n",
    "# # Extract x, y, z coordinates\n",
    "# x_coords = inputs[:, 0].numpy()\n",
    "# y_coords = inputs[:, 1].numpy()\n",
    "# z_coords = inputs[:, 2].numpy()\n",
    "\n",
    "# # Create 3D plot\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# # Plot each point and annotate with corresponding word\n",
    "# for x, y, z, word in zip(x_coords, y_coords, z_coords, words):\n",
    "#     ax.scatter(x, y, z)\n",
    "#     ax.text(x, y, z, word, fontsize=10)\n",
    "\n",
    "# # Set labels for axes\n",
    "# ax.set_xlabel('X')\n",
    "# ax.set_ylabel('Y')\n",
    "# ax.set_zlabel('Z')\n",
    "\n",
    "# plt.title('3D Plot of Word Embeddings')\n",
    "# plt.show()\n",
    "\n",
    "# # Create 3D plot with vectors from origin to each point, using different colors\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# # Define a list of colors for the vectors\n",
    "# colors = ['r', 'g', 'b', 'c', 'm', 'y', 'r']\n",
    "\n",
    "# # Plot each vector with a different color and annotate with the corresponding word\n",
    "# for (x, y, z, word, color) in zip(x_coords, y_coords, z_coords, words, colors):\n",
    "#     # Draw vector from origin to the point (x, y, z) with specified color and smaller arrow length ratio\n",
    "#     ax.quiver(0, 0, 0, x, y, z, color=color, arrow_length_ratio=0.05)\n",
    "#     ax.text(x, y, z, word, fontsize=10, color=color)\n",
    "\n",
    "# # Set labels for axes\n",
    "# ax.set_xlabel('X')\n",
    "# ax.set_ylabel('Y')\n",
    "# ax.set_zlabel('Z')\n",
    "\n",
    "# # Set plot limits to keep arrows within the plot boundaries\n",
    "# ax.set_xlim([0, 1])\n",
    "# ax.set_ylim([0, 1])\n",
    "# ax.set_zlim([0, 1])\n",
    "\n",
    "# plt.title('3D Plot of Word Embeddings with Colored Vectors')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02647df1-8652-4146-8198-4e06305ae662",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Now, we can extend this computation to\n",
    "calculate attention weights and context vectors for all inputs.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a217eb52-0076-4530-aaf5-feb9c953f6ac",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "First, we add an additional for-loop to compute the\n",
    "dot products for all pairs of inputs.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "194bbc5d-c3d0-473b-abf1-7b2da78360b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = torch.empty(6, 6)\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d35fe1-62d8-449e-9af9-3c172f651cab",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Each element in the preceding tensor represents an attention score between each pair of\n",
    "inputs.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a453c3-e331-4151-ab25-e2d5b4f48fe0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "When computing the preceding attention score tensor, we used for-loops in Python.\n",
    "                                                            \n",
    "However, for-loops are generally slow, and we can achieve the same results using matrix\n",
    "multiplication:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "09726f95-aac8-4179-b18b-a458a95ff19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = inputs @ inputs.T\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e09df7-6476-45d7-a399-8552a3da2402",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "We now normalize each row so that the values in\n",
    "each row sum to 1:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "01e2c471-a8de-4662-81eb-d0003f361015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1df0e84-27a7-4dd3-9a1d-da79984ecb01",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "In the context of using PyTorch, the dim parameter in functions like torch.softmax specifies\n",
    "the dimension of the input tensor along which the function will be computed. \n",
    "\n",
    "By setting\n",
    "dim=-1, we are instructing the softmax function to apply the normalization along the last\n",
    "dimension of the attn_scores tensor. \n",
    "\n",
    "If attn_scores is a 2D tensor (for example, with a\n",
    "shape of [rows, columns]), dim=-1 will normalize across the columns so that the values in\n",
    "each row (summing over the column dimension) sum up to 1.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c0375f-172b-42a4-9c13-df18b18507b5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Let's briefly verify that\n",
    "the rows indeed all sum to 1:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dfd49b79-b192-49dd-bb70-d306a4d862e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 2 sum: 1.0\n",
      "All row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "row_2_sum = sum([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
    "print(\"Row 2 sum:\", row_2_sum)\n",
    "print(\"All row sums:\", attn_weights.sum(dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e424e24d-0ec3-47a1-8ac7-48f1d5f162ce",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "In the third and last step, we now use these attention weights to compute all context\n",
    "vectors via matrix multiplication:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4f34105e-ddbe-4ddd-a908-ad5e9ad2ce4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "all_context_vecs = attn_weights @ inputs\n",
    "print(all_context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9716a2a-25ea-479e-8bc2-e577de9802ac",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "We can double-check that the code is correct by comparing the 2nd row with the context\n",
    "vector z(2) calculated previously\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8579a6c5-5954-4e8c-912b-ea5c0b9610f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous 2nd context vector: tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "print(\"Previous 2nd context vector:\", context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc7b357-e782-4ac5-88d2-91c4c482ea00",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Based on the result, we can see that the previously calculated context_vec_2 matches the\n",
    "second row in the previous tensor exactly\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6283261-99ef-4bb1-8ab9-138c5d976287",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "This concludes the code walkthrough of a simple self-attention mechanism.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371215c1-d5e4-40d5-80f2-82d3258b6305",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83aed3aa-32f4-4159-998a-4bd53900c9c2",
   "metadata": {},
   "source": [
    "## IMPLEMENTING SELF ATTENTION WITH TRAINABLE WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6fef3bc5-015d-4c1e-94dd-46d4c2dd61d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd3b53f-a091-46a5-a10d-b8e7e210b4e2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Let's begin by defining a few variables:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f43a95-c4dc-4698-b543-a4d9f5fa4c0f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "#A The second input element\n",
    "\n",
    "#B The input embedding size, d=3\n",
    "\n",
    "\n",
    "#C The output embedding size, d_out=2\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "949845cd-4ad3-4e23-83c4-d95f19534314",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1] #A\n",
    "d_in = inputs.shape[1] #B\n",
    "d_out = 2 #C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7ff47a-005a-486d-b93e-05aa840932e2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Note that in GPT-like models, the input and output dimensions are usually the same. \n",
    "\n",
    "But for illustration purposes, to better follow the computation, we choose different input (d_in=3)\n",
    "and output (d_out=2) dimensions here.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6d03b5-8607-4f5f-aec3-262710f089be",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Next, we initialize the three weight matrices Wq, Wk and Wv\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1bbeedbd-a0d3-4b37-91d6-952396b5b02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0c97a8-cb91-41ae-812e-38fa9ded6f81",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Note that we are setting requires_grad=False to reduce clutter in the outputs for\n",
    "illustration purposes. \n",
    "\n",
    "If we were to use the weight matrices for model training, we\n",
    "would set requires_grad=True to update these matrices during model training.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23f955c-aea0-429a-8fd9-7e464c789b35",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Next, we compute the query, key, and value vectors as shown earlier\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b11a73bc-3fc9-4245-bbf9-71ae5f531ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "print(query_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ff14ce-7497-4284-8761-9127a4617454",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "As we can see based on the output for the query, this results in a 2-dimensional vector. \n",
    "\n",
    "This is because: we set the number of columns of the corresponding weight matrix, via d_out, to 2:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41dcda9-069c-4355-922e-03da48f196f5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Even though our temporary goal is to only compute the one context vector z(2),  we still\n",
    "require the key and value vectors for all input elements. \n",
    "\n",
    "This is because they are involved in computing the attention weights with respect to the query q(2)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77601be-91f5-4605-bbc4-72447c0e40eb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "We can obtain all keys and values via matrix multiplication:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "92ddea8c-fa32-4343-bd18-ea39aca632a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n",
      "queries.shape: torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "queries = inputs @ W_query\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)\n",
    "print(\"queries.shape:\", query.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73786a41-f685-4dfc-b572-4248074d26c0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "As we can tell from the outputs, we successfully projected the 6 input tokens from a 3D\n",
    "onto a 2D embedding space:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee43fd27-15c6-49d3-94f2-2b9b26db31be",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "First, let's compute the attention score ω22</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d0005d54-4ee6-466f-85b9-a6bb7e97954c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "keys_2 = keys[1]  #A\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(attn_score_22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f815b0f9-1fac-434b-b494-125c322fe33e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Again, we can generalize this computation to all attention scores via matrix multiplication:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9bfbb1cf-f88f-4c2a-bfec-99cd693c14cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2 = query_2 @ keys.T  #All attention scores for given query\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ff3aafc2-c70b-4d64-9703-974754b91d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9231, 1.3545, 1.3241, 0.7910, 0.4032, 1.1330],\n",
      "        [1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440],\n",
      "        [1.2544, 1.8284, 1.7877, 1.0654, 0.5508, 1.5238],\n",
      "        [0.6973, 1.0167, 0.9941, 0.5925, 0.3061, 0.8475],\n",
      "        [0.6114, 0.8819, 0.8626, 0.5121, 0.2707, 0.7307],\n",
      "        [0.8995, 1.3165, 1.2871, 0.7682, 0.3937, 1.0996]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = queries @ keys.T   #omega\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e88aef-9667-4204-9d1a-8776a951983b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "We compute the attention weights by scaling the\n",
    "attention scores and using the softmax function we used earlier. \n",
    "\n",
    "The difference to earlier is\n",
    "that we now scale the attention scores by dividing them by the square root of the\n",
    "embedding dimension of the keys. \n",
    "\n",
    "Note that taking the square root is mathematically the\n",
    "same as exponentiating by 0.5:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9de0f39d-8069-435c-a790-1ba2d65e1173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "d_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
    "print(attn_weights_2)\n",
    "print(d_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9897345c-863d-447b-b492-decae306d633",
   "metadata": {},
   "source": [
    "### WHY DIVIDE BY SQRT (DIMENSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd336bd-b6cc-401e-80ce-213f9dc6de75",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Reason 1: For stability in learning \n",
    "    \n",
    "The softmax function is sensitive to the magnitude of its input. when the inputs are large, the differences between the exponential values of each input become much more pronounced. This causes the softmax output to become \"peaky\", where the highest value receives almost all the probability mass, and the rest receive very little.\n",
    "\n",
    "In attention mechanism, particularly in transofrmers, if the dot product between query and key vecotrs become too large, the attention scores can become very large. this result is very sharp softmax distribution , making the model overly confident in one particular \"key.\" such sharp distribution can make learning unstable,\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85ada0c-aa14-4b8f-85d3-813a2df20f15",
   "metadata": {},
   "source": [
    "### BUT WHY SQRT?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dad686b-1d69-42d5-8321-cac71bb3eb06",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Reason 2: To make the variance of the dot product stable\n",
    "\n",
    "The dot product of Q and K increases the Variance because multiplying two random numbers increases the variance.\n",
    "\n",
    "The increase in variance grows with the dimension.\n",
    "\n",
    "Dividing by sqrt (dimension) keeps the variance close to 1\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "44cbb577-adbd-4446-9dab-3f858930df57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "## FInal step \n",
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea0612b-2839-456f-91b0-7e7f8153ec80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0718f1bf-b659-41a7-9648-9792cedad71c",
   "metadata": {},
   "source": [
    "## IMPLEMENTING A COMPACT SELF ATTENTION PYTHON CLASS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e96e7b-5048-43fa-ae5a-5dafe408bbcb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "In the previous sections, we have gone through a lot of steps to compute the self-attention\n",
    "outputs. \n",
    "\n",
    "This was mainly done for illustration purposes so we could go through one step at\n",
    "a time. \n",
    "\n",
    "In practice, with the LLM implementation in the next chapter in mind, it is helpful to\n",
    "organize this code into a Python class as follows:\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b2a053ff-b937-46dd-b0d7-4201854f7198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "        \n",
    "        attn_scores = queries @ keys.T # omega\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6123929-cd27-4621-850c-f715451385fa",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "In this PyTorch code, SelfAttention_v1 is a class derived from nn.Module, which is a\n",
    "fundamental building block of PyTorch models, which provides necessary functionalities for\n",
    "model layer creation and management.    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3511243c-8905-4523-8c86-389abec2d37e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "The __init__ method initializes trainable weight matrices (W_query, W_key, and\n",
    "W_value) for queries, keys, and values, each transforming the input dimension d_in to an\n",
    "output dimension d_out.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032c07c1-db26-49d4-a91c-7244e5c886b3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "During the forward pass, using the forward method, we compute the attention scores\n",
    "(attn_scores) by multiplying queries and keys, normalizing these scores using softmax.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca724c7-57b4-4b7a-8b74-2270dbc1caec",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Finally, we create a context vector by weighting the values with these normalized attention\n",
    "scores.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "2d79c10a-3add-4055-bf2c-082c167e3c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c31d5d6-435b-4aad-9a67-8dbd34e340e3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Since inputs contains six embedding vectors, we get a matrix storing the six\n",
    "context vectors, as shown in the above result. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a036ae62-36e0-4e5c-b3e0-e562fb4dad05",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "As a quick check, notice how the second row ([0.3061, 0.8210]) matches the contents of\n",
    "context_vec_2 in the previous section.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae54943-5847-4034-b8f3-54157027f65c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "We can improve the SelfAttention_v1 implementation further by utilizing PyTorch's\n",
    "nn.Linear layers, which effectively perform matrix multiplication when the bias units are\n",
    "disabled. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de5ce7f-d55b-449a-82f9-e14ebc592ea4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Additionally, a significant advantage of using nn.Linear instead of manually\n",
    "implementing nn.Parameter(torch.rand(...)) is that nn.Linear has an optimized weight\n",
    "initialization scheme, contributing to more stable and effective model training.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "1526ecab-e05f-41ac-a73f-88e2f9859bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a832de32-9836-4c09-8c77-63ccde0146cc",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "You can use the SelfAttention_v2 similar to SelfAttention_v1:\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "4f09a162-9f30-4a86-a6ed-c14ccce147ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940fe91a-5501-4b24-a237-6de233244925",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Note that SelfAttention_v1 and SelfAttention_v2 give different outputs because they\n",
    "use different initial weights for the weight matrices since nn.Linear uses a more\n",
    "sophisticated weight initialization scheme.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ea2a83-17b0-4403-bab7-2b81cc114d63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b85677d1-a71f-4301-b6a0-def2baafb5a0",
   "metadata": {},
   "source": [
    "## CAUSAL ATTENTION "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b815577-90b8-4ff4-b681-cbe334441b14",
   "metadata": {},
   "source": [
    "### HIDING FUTURE WORDS WITH CASUAL ATTENTION "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3ad130-f187-41bb-b3c8-15fad17a7b72",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Let's work with the attention scores and weights from the previous section to code the causal attention mechanism.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a126aa-7257-4b65-87d3-17136063bd61",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "In the first step illustrated in Figure 3.20, we compute the attention weights using the\n",
    "softmax function as we have done in previous sections:    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ea474d-018d-4bd7-a1a8-5a2c741425b1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Reuse the query and key weight matrices of the SelfAttention_v2 object from the previous section for\n",
    "convenience\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c9620f79-3c14-4ae3-b154-43fdb93e3684",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "9fbb16f9-5a03-46eb-b06d-4b86e4345eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries = sa_v2.W_query(inputs)  #A\n",
    "keys = sa_v2.W_key(inputs)\n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a4edd7-18ac-42d4-ad02-5654f8990768",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "We can now use PyTorch's tril function to create a mask\n",
    "where the values above the diagonal are zero:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "cde5bf22-50a7-4bd3-b8a5-d6ec05f56a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "context_length = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "print(mask_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c572199-06dd-4e92-88a6-fa0de408a768",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Now, we can multiply this mask with the attention weights to zero out the values above the\n",
    "diagonal:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "0ca138a3-a221-45f2-8e6f-e4aa07363c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_simple = attn_weights * mask_simple\n",
    "print(masked_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9926d3-9db9-46eb-8057-22d7c172382c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "As we can see, the elements above the diagonal are successfully zeroed out\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a13f27-07d3-4358-939f-37668d251583",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "The third step is to renormalize the attention weights to sum up to 1 again in\n",
    "each row. \n",
    "\n",
    "We can achieve this by dividing each element in each row by the sum in each\n",
    "row:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "800f8059-07d6-4aa5-b73d-c81905387455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "row_sums = masked_simple.sum(dim=1, keepdim = True)\n",
    "masked_simple_norm = masked_simple / row_sums\n",
    "print(masked_simple_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27364014-456a-499d-b0f9-e07e98fca9e9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "The result is an attention weight matrix where the attention weights above the diagonal are\n",
    "zeroed out and where the rows sum to 1.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78252705-cbf9-4f85-b616-bfa2476d0c50",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "While we could be technically done with implementing causal attention at this point, we can\n",
    "take advantage of a mathematical property of the softmax function. \n",
    "\n",
    "We can implement the computation of the masked attention weights more efficiently in fewer steps.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a566f10-461f-4f32-a14f-5a2308c0b74c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "The softmax function converts its inputs into a probability distribution. \n",
    "\n",
    "When negative\n",
    "infinity values (-∞) are present in a row, the softmax function treats them as zero\n",
    "probability. \n",
    "\n",
    "(Mathematically, this is because e\n",
    "-∞ approaches 0.)\n",
    "\n",
    "\n",
    "We can implement this more efficient masking \"trick\" by creating a mask with 1's above\n",
    "the diagonal and then replacing these 1's with negative infinity (-inf) values:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "ec378072-7e15-421a-8130-ed8c4c2c0e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e488265c-970d-4ba3-9566-b709826e3e8f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Now, all we need to do is apply the softmax function to these masked results, and we are\n",
    "done.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "c8102da9-cdee-4c1e-a704-eba8e11e15cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791faa5b-8984-4076-8366-7a0338809da3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "As we can see based on the output, the values in each row sum to 1, and no further\n",
    "normalization is necessary.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09c9d55-329f-46f3-ab88-c749569bfa2e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Masking in Transformers sets scores for future tokens to a large negative value, making their influence in the softmax calculation effectively zero. \n",
    "\n",
    "The softmax function then recalculates attention weights only among the unmasked tokens. \n",
    "\n",
    "This process ensures no information leakage from masked tokens, focusing the model solely on the intended data.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605ec7d4-29d4-473e-a20b-88b3f86a650b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "We could now use the modified attention weights to compute the context vectors via\n",
    "context_vec = attn_weights @ values.\n",
    "\n",
    "However, in the next section,\n",
    "we first cover another minor tweak to the causal attention mechanism that is useful for\n",
    "reducing overfitting when training LLMs.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e27dfb-5877-4569-80d3-6860e687d0f0",
   "metadata": {},
   "source": [
    "### MASKING ADDITIONAL ATTENTION WEIGHTS WITH DROPOUT "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8378edbc-2805-477e-a095-a86dfc62ebb3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "In the following code example, we use a dropout rate of 50%, which means masking out\n",
    "half of the attention weights.\n",
    "\n",
    "When we train the GPT model in later chapters, we will use a\n",
    "lower dropout rate, such as 0.1 or 0.2.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dc67f0-9991-4d34-924e-f156a09b208b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "In the following code, we apply PyTorch's dropout implementation first to a 6×6 tensor\n",
    "consisting of ones for illustration purposes:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "da9b8c09-b635-40d3-8fdd-8a571d302de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5) #A\n",
    "example = torch.ones(6, 6) #B\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c271ba3-424c-496b-87ad-8a179df6def1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "When applying dropout to an attention weight matrix with a rate of 50%, half of the\n",
    "elements in the matrix are randomly set to zero. \n",
    "\n",
    "To compensate for the reduction in active\n",
    "elements, the values of the remaining elements in the matrix are scaled up by a factor of\n",
    "1/0.5 =2. \n",
    "\n",
    "This scaling is crucial to maintain the overall balance of the attention weights,\n",
    "ensuring that the average influence of the attention mechanism remains consistent during\n",
    "both the training and inference phases.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746d3372-1524-4eda-8527-2cafba75f3aa",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Now, let's apply dropout to the attention weight matrix itself:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "0e2faad6-e9c3-45a7-9865-5104e907bdcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7599, 0.6194, 0.6206, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4921, 0.4925, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3966, 0.0000, 0.3775, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3327, 0.3331, 0.3084, 0.3331, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3103526-791a-4308-8ef4-7989fa5e9bf6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "As we can see above, the resulting attention weight matrix now has additional elements zeroed out and the\n",
    "remaining ones rescaled.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062a87f9-8578-48d6-a0c8-a854a2819ee2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Having gained an understanding of causal attention and dropout masking, we will\n",
    "develop a concise Python class in the following section. \n",
    "\n",
    "This class is designed to facilitate\n",
    "the efficient application of these two techniques.\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6dc736-f7f0-4211-a042-b10e049e7abe",
   "metadata": {},
   "source": [
    "### IMPLEMENTING A COMPACT CAUSAL ATTENTION CLASS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6786c53-5eb7-4c87-b52b-d091526e8126",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "In this section, we will now incorporate the causal attention and dropout modifications into\n",
    "the SelfAttention Python class we developed in section 3.4. \n",
    "\n",
    "This class will then serve as a\n",
    "template for developing multi-head attention in the upcoming section.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399ad834-5e1e-4008-910f-e50ee6f6b810",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Before we begin, one more thing is to ensure that the code can handle batches\n",
    "consisting of more than one input. \n",
    "\n",
    "This will ensure that the CausalAttention class supports the batch\n",
    "outputs produced by the data loader we implemented earlier.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02060549-826e-433e-887d-ec4539c73e25",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "For simplicity, to simulate such batch inputs, we duplicate the input text example:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a66794-764a-474b-ae83-79858799ab0c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    " 2 inputs with 6 tokens each, and each token has embedding dimension 3\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "8c6e8a00-fef8-46d7-9e50-c41b90b306a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch  = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932d66a9-ead0-4b0a-8268-923d1b2a8d2f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "This results in a 3D tensor consisting of 2 input texts with 6 tokens each, where each token\n",
    "is a 3-dimensional embedding vector.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9dcf21-9fa5-48d7-bfc1-a3a7f468e9f2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "The following CausalAttention class is similar to the SelfAttention class we\n",
    "implemented earlier, except that we now added the dropout and causal mask components\n",
    "as highlighted in the following code.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d2c292-8bb1-49cf-bd70-451d625b517b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Step 1: Compared to the previous SelfAttention_v1 class, we added a dropout layer.\n",
    "    \n",
    "Step 2: The register_buffer call is also a new addition (more information is provided in the following text).\n",
    "\n",
    "Step 3:  We transpose dimensions 1 and 2, keeping the batch dimension at the first position (0).\n",
    "\n",
    "Step 4: In PyTorch, operations with a trailing underscore are performed in-place, avoiding unnecessary memory\n",
    "copies\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "ed664d9a-e0ed-4832-b942-88481351256d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length,\n",
    "                 dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout) # New\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1)) # New\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape # New batch dimension b\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(1, 2) # Changed transpose\n",
    "        attn_scores.masked_fill_(  # New, _ ops are in-place\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)  # `:num_tokens` to account for cases where the number of tokens in the batch is smaller than the supported context_size\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        attn_weights = self.dropout(attn_weights) # New\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86231460-5db8-4e1a-a581-c22a161f7a8a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "The use of register_buffer in\n",
    "PyTorch is not strictly necessary for all use cases but offers several advantages here. \n",
    "\n",
    "For\n",
    "instance, when we use the CausalAttention class in our LLM, buffers are automatically\n",
    "moved to the appropriate device (CPU or GPU) along with our model, which will be relevant\n",
    "when training the LLM in future chapters. \n",
    "\n",
    "This means we don't need to manually ensure\n",
    "these tensors are on the same device as your model parameters, avoiding device mismatch\n",
    "errors.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272ae881-8054-4955-aeee-de7e82a4ff7d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "We can use the CausalAttention class as follows, similar to SelfAttention previously:\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "0c852f26-78c8-4b4f-b0a2-da253e0b7398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b64e66-3ffb-434d-8f33-0d85df51324c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "As we can see, the resulting context vector is a 3D tensor where each token is now represented by a 2D\n",
    "embedding:\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d609bf2f-5dd3-4c33-a5c0-7973ac41f72c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "In the next section, we will expand on this concept\n",
    "and implement a multi-head attention module, that implements several of such causal\n",
    "attention mechanisms in parallel.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6562bc-caca-40eb-b742-41df125cfcc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

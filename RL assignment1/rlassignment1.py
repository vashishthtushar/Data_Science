# -*- coding: utf-8 -*-
"""RLAssignment1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Thbf3wNR_Em81UgkuRdXVGaWqUxCmvxC
"""

!pip install numpy==1.23.5 --quiet
import os
os.kill(os.getpid(), 9)  # Force restart the Colab kernel

import gym
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
import random
from collections import deque
import matplotlib.pyplot as plt

"""### Q1. DQN"""

import gym
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import random
from collections import deque
import matplotlib.pyplot as plt

# Hyperparameters
ENV_NAME = "CartPole-v1"
GAMMA = 0.99
LR = 1e-3
BATCH_SIZE = 64
BUFFER_SIZE = 10000
MIN_REPLAY_SIZE = 1000
TARGET_UPDATE_FREQ = 10
EPSILON_START = 1.0
EPSILON_END = 0.02
EPSILON_DECAY = 10000
EPISODES = 1500

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Neural Network for Q-value approximation
class DQN(nn.Module):
    def __init__(self, obs_dim, n_actions):
        super(DQN, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(obs_dim, 128),
            nn.ReLU(),
            nn.Linear(128, n_actions)
        )

    def forward(self, x):
        return self.net(x)

# Epsilon-greedy action selection
def select_action(model, state, epsilon):
    if random.random() < epsilon:
        return random.randint(0, env.action_space.n - 1)
    else:
        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)
        with torch.no_grad():
            q_values = model(state)
        return q_values.argmax().item()

# Initialize environment and networks
env = gym.make(ENV_NAME)
obs_dim = env.observation_space.shape[0]
n_actions = env.action_space.n

policy_net = DQN(obs_dim, n_actions).to(device)
target_net = DQN(obs_dim, n_actions).to(device)
target_net.load_state_dict(policy_net.state_dict())
target_net.eval()

optimizer = optim.Adam(policy_net.parameters(), lr=LR)
replay_buffer = deque(maxlen=BUFFER_SIZE)

# Logging
episode_rewards = []
moving_avg_rewards = []

# Fill replay buffer initially
state = env.reset()
for _ in range(MIN_REPLAY_SIZE):
    action = env.action_space.sample()
    next_state, reward, done, _ = env.step(action)
    replay_buffer.append((state, action, reward, next_state, done))
    if done:
        state = env.reset()
    else:
        state = next_state

# Training loop
steps_done = 0
for episode in range(EPISODES):
    state = env.reset()
    done = False
    total_reward = 0

    while not done:
        epsilon = EPSILON_END + (EPSILON_START - EPSILON_END) * \
                  np.exp(-1. * steps_done / EPSILON_DECAY)
        action = select_action(policy_net, state, epsilon)
        next_state, reward, done, _ = env.step(action)
        replay_buffer.append((state, action, reward, next_state, done))
        state = next_state
        total_reward += reward
        steps_done += 1

        # Sample mini-batch from replay buffer
        batch = random.sample(replay_buffer, BATCH_SIZE)
        states, actions, rewards, next_states, dones = zip(*batch)

        states = torch.tensor(states, dtype=torch.float32).to(device)
        actions = torch.tensor(actions).unsqueeze(1).to(device)
        rewards = torch.tensor(rewards).to(device)
        next_states = torch.tensor(next_states, dtype=torch.float32).to(device)
        dones = torch.tensor(dones, dtype=torch.float32).to(device)

        # Compute Q-targets
        q_values = policy_net(states).gather(1, actions).squeeze()
        with torch.no_grad():
            max_next_q_values = target_net(next_states).max(1)[0]
            targets = rewards + GAMMA * max_next_q_values * (1 - dones)

        # Compute loss
        loss = nn.MSELoss()(q_values, targets)

        # Optimize model
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    # Update target network
    if episode % TARGET_UPDATE_FREQ == 0:
        target_net.load_state_dict(policy_net.state_dict())

    # Log results
    episode_rewards.append(total_reward)
    moving_avg = np.mean(episode_rewards[-50:])
    moving_avg_rewards.append(moving_avg)
    print(f"Episode {episode+1}, Reward: {total_reward}, Moving Avg: {moving_avg:.2f}, Epsilon: {epsilon:.2f}")

# Save model
torch.save(policy_net.state_dict(), "dqn_cartpole.pt")

# Plot results
plt.figure(figsize=(10,5))
plt.plot(episode_rewards, label="Reward")
plt.plot(moving_avg_rewards, label="Moving Avg (50)")
plt.xlabel("Episodes")
plt.ylabel("Reward")
plt.title("DQN on CartPole-v1")
plt.legend()
plt.savefig("dqn_reward_plot.png")
plt.show()

env.close()

"""### Q2. REINFORCE"""

# Hyperparameters
ENV_NAME = 'CartPole-v1'
LR = 1e-3
GAMMA = 0.99
EPISODES = 1500

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Policy Network
class PolicyNetwork(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(PolicyNetwork, self).__init__()
        self.fc = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim),
            nn.Softmax(dim=-1)
        )

    def forward(self, x):
        return self.fc(x)

# Select action using softmax policy
def select_action(policy_net, state):
    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)
    probs = policy_net(state)
    dist = torch.distributions.Categorical(probs)
    action = dist.sample()
    log_prob = dist.log_prob(action)
    return action.item(), log_prob

# Initialize environment and network
env = gym.make(ENV_NAME)
obs_dim = env.observation_space.shape[0]
n_actions = env.action_space.n

policy_net = PolicyNetwork(obs_dim, n_actions).to(device)
optimizer = optim.Adam(policy_net.parameters(), lr=LR)

# Logging
episode_rewards = []
moving_avg_rewards = []

# Training loop
for episode in range(EPISODES):
    state = env.reset()
    done = False
    log_probs = []
    rewards = []
    total_reward = 0

    while not done:
        action, log_prob = select_action(policy_net, state)
        next_state, reward, done, _ = env.step(action)
        log_probs.append(log_prob)
        rewards.append(reward)
        state = next_state
        total_reward += reward

    # Compute return (G) for each step
    returns = []
    G = 0
    for r in reversed(rewards):
        G = r + GAMMA * G
        returns.insert(0, G)
    returns = torch.tensor(returns, dtype=torch.float32).to(device)

    # Normalize returns (optional but helps)
    returns = (returns - returns.mean()) / (returns.std() + 1e-8)

    # Compute policy loss
    policy_loss = []
    for log_prob, G in zip(log_probs, returns):
        policy_loss.append(-log_prob * G)
    policy_loss = torch.stack(policy_loss).sum()

    # Backpropagation
    optimizer.zero_grad()
    policy_loss.backward()
    optimizer.step()

    # Logging
    episode_rewards.append(total_reward)
    moving_avg = np.mean(episode_rewards[-50:])
    moving_avg_rewards.append(moving_avg)
    print(f"Episode {episode+1}, Reward: {total_reward}, Moving Avg: {moving_avg:.2f}")

# Save model
torch.save(policy_net.state_dict(), "reinforce_cartpole.pt")

# Plot
plt.figure(figsize=(10,5))
plt.plot(episode_rewards, label="Reward")
plt.plot(moving_avg_rewards, label="Moving Avg (50)")
plt.xlabel("Episodes")
plt.ylabel("Reward")
plt.title("REINFORCE on CartPole-v1")
plt.legend()
plt.savefig("reinforce_reward_plot.png")
plt.show()

env.close()

"""### Q3. A2C"""

# Hyperparameters
ENV_NAME = 'CartPole-v1'
GAMMA = 0.99
LR = 1e-3
EPISODES = 1500

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Actor Network
class Actor(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(Actor, self).__init__()
        self.fc = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim),
            nn.Softmax(dim=-1)
        )

    def forward(self, x):
        return self.fc(x)

# Critic Network
class Critic(nn.Module):
    def __init__(self, state_dim):
        super(Critic, self).__init__()
        self.fc = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 1)
        )

    def forward(self, x):
        return self.fc(x)

# Select action using actor
def select_action(actor, state):
    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)
    probs = actor(state)
    dist = torch.distributions.Categorical(probs)
    action = dist.sample()
    log_prob = dist.log_prob(action)
    return action.item(), log_prob

# Setup environment
env = gym.make(ENV_NAME)
state_dim = env.observation_space.shape[0]
action_dim = env.action_space.n

actor = Actor(state_dim, action_dim).to(device)
critic = Critic(state_dim).to(device)

actor_optimizer = optim.Adam(actor.parameters(), lr=LR)
critic_optimizer = optim.Adam(critic.parameters(), lr=LR)

# Logging
episode_rewards = []
moving_avg_rewards = []

# Training loop
for episode in range(EPISODES):
    state = env.reset()
    done = False
    total_reward = 0

    while not done:
        action, log_prob = select_action(actor, state)
        next_state, reward, done, _ = env.step(action)

        # Convert states to tensors
        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)
        next_state_tensor = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0).to(device)

        # TD Target and TD Error
        with torch.no_grad():
            target_value = reward + GAMMA * critic(next_state_tensor) * (1 - int(done))
        value = critic(state_tensor)
        td_error = target_value - value

        # Update Critic
        critic_loss = td_error.pow(2)
        critic_optimizer.zero_grad()
        critic_loss.backward()
        critic_optimizer.step()

        # Update Actor
        actor_loss = -log_prob * td_error.detach()
        actor_optimizer.zero_grad()
        actor_loss.backward()
        actor_optimizer.step()

        state = next_state
        total_reward += reward

    episode_rewards.append(total_reward)
    moving_avg = np.mean(episode_rewards[-50:])
    moving_avg_rewards.append(moving_avg)
    print(f"Episode {episode+1}, Reward: {total_reward}, Moving Avg: {moving_avg:.2f}")

# Save models
torch.save(actor.state_dict(), "a2c_actor_cartpole.pt")
torch.save(critic.state_dict(), "a2c_critic_cartpole.pt")

# Plot
plt.figure(figsize=(10,5))
plt.plot(episode_rewards, label="Reward")
plt.plot(moving_avg_rewards, label="Moving Avg (50)")
plt.xlabel("Episodes")
plt.ylabel("Reward")
plt.title("Actor-Critic (A2C) on CartPole-v1")
plt.legend()
plt.savefig("a2c_reward_plot.png")
plt.show()

env.close()
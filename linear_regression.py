# -*- coding: utf-8 -*-
"""Linear_regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14Ho48vLDjlkZ7Lh5K7HWiy2X4Z386E7T
"""

# House Pricing Dataset
from sklearn.datasets import load_boston

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Recreate the data and target as per the previous cell
data_url = "http://lib.stat.cmu.edu/datasets/boston"
raw_df = pd.read_csv(data_url, sep="\s+", skiprows=22, header=None)
data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])
target = raw_df.values[1::2, 2]

# Define the feature names according to the dataset documentation
feature_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']

# Create the DataFrame from the 'data' array
# We are creating a DataFrame from the 'data' array which has 12 columns
dataset = pd.DataFrame(data)

# Assign the correct column names to the DataFrame features (first 12 columns)
# We only have 12 features in our 'data' array, so we only assign the first 12 names
dataset.columns = feature_names[:13]


# Display the head of the dataset to verify
dataset.head()

dataset['price'] = target

dataset.head()

## Dividing the dataset into independent and dependent features
X = dataset.iloc[:,:-1] ## Independent features (we are just sckiping the last index)
y = dataset.iloc[:, -1] ## Dependent Features (the last one only is the dependent feature)

y.head()

## Linear Regression
from sklearn.linear_model import LinearRegression  ## For Linear regression model
from sklearn.model_selection import train_test_split  ## For Splitting data into training and
from sklearn.model_selection import cross_val_score ## For Cross Validation

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.33, random_state=42)

lin_reg = LinearRegression()
lin_reg.fit(X_train,y_train)
mse = cross_val_score(lin_reg,X_train,y_train,scoring='neg_mean_squared_error',cv=5)
mean_mse = np.mean(mse)
print(mean_mse)

# Ridge Regression (L2 Regularization)
from sklearn.linear_model import Ridge
from sklearn.model_selection import GridSearchCV
ridge=Ridge()

params={'alpha':[1e-15,1e-10,1e-8,1e-3,1e-2,1,5,10,20,30,35,40,45,50,55,100]}
ridge_regressor=GridSearchCV(ridge,params,scoring='neg_mean_squared_error',cv=10)
ridge_regressor.fit(X_train,y_train)

print(ridge_regressor.best_params_)
print(ridge_regressor.best_score_)

# Lasso Regression (L Regularization)
from sklearn.linear_model import Lasso
from sklearn.model_selection import GridSearchCV
lasso=Lasso()

params={'alpha':[1e-15,1e-10,1e-8,1e-3,1e-2,1,5,10,20,30,35,40,45,50,55,100]}
lasso_regressor=GridSearchCV(lasso,params,scoring='neg_mean_squared_error',cv=10)
lasso_regressor.fit(X_train,y_train)

print(lasso_regressor.best_params_)
print(lasso_regressor.best_score_)

y_pred = lin_reg.predict(X_test)
from sklearn.metrics import r2_score
r2_score(y_pred, y_test)

"""# Logistic Regression"""

## Logistic Regression
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import load_breast_cancer

df = load_breast_cancer()
### Independent features
X = pd.DataFrame(df['data'],columns = df['feature_names'])

X.head()

## Dependent Feature
y = pd.DataFrame(df['target'],columns = ['Target'])
y.head()

y['Target'].value_counts()

## Train_test_split
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.33,random_state=42)

params=[{'C':[1,5,10]},{'max_iter':[100,150]}]

model1 = LogisticRegression(C=100,max_iter=100)

model = GridSearchCV(model1,param_grid=params,scoring='f1',cv=5)

model.fit(X_train,y_train)

model.best_params_

model.best_score_

y_pred = model.predict(X_test)

y_pred

from sklearn.metrics import confusion_matrix,classification_report,accuracy_score

confusion_matrix(y_test,y_pred)

accuracy_score(y_test,y_pred)

print(classification_report(y_test,y_pred))

